# PySpark and Spark-SQL

<img src="https://github.com/sularaperera/PySpark-Spark-SQL/blob/main/PySpark.png" width="400"></img>
<img src="https://github.com/sularaperera/PySpark-Spark-SQL/blob/main/SparkSQL.jpeg" width="400"></img>

PySpark is a great language for performing exploratory data analysis at scale, building machine learning pipelines, and creating ETLs for a data platform. If you’re already familiar with Python and libraries such as Pandas, then PySpark is a great language to learn in order to create more scalable analyses and pipelines. The goal of this repo is to show how to get up and running with PySpark and to perform common tasks.

## PySpark:
PySpark is the Python API for Spark. It lets you interface with Spark's distributed computing capabilities using Python. With PySpark, you can create Spark RDDs (Resilient Distributed Datasets), which are the fundamental data structure of Spark, and perform various transformations and actions on them. PySpark supports most of Spark's features, such as Spark SQL, MLlib for machine learning, GraphX for graph processing, and Spark Streaming. PySpark is used for processing large volumes of data, data analytics, machine learning, and many other big data processing tasks. It is particularly popular among data scientists and data engineers who prefer writing Python code.

## Spark SQL:
Spark SQL is a module in Apache Spark that integrates relational processing with Spark's functional programming API. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. It allows querying data via SQL as well as the Apache Hive variant of SQL — called the Hive Query Language (HQL) — and it supports many sources of data including Hive tables, Parquet, and JSON. Spark SQL allows you to execute SQL queries in Spark and can also be used to read data from an existing Hive installation. When you use Spark SQL, you can seamlessly mix SQL queries with Spark's programmatic data manipulations, leveraging Spark's distributed computation power.

Both PySpark and Spark SQL are designed to scale up from a single server to thousands of machines, each offering local computation and storage. They make it easier to handle big data processing and analytics.
